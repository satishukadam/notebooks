{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "#import NLTK libraries\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# import GUI libraries\n",
    "\n",
    "\n",
    "# read data file\n",
    "df = pd.read_excel('D:/Intelliza_Projects/NLP/goodbrief.xlsx')\n",
    "\n",
    "# append the data into comtent field\n",
    "df['content'] = df.iloc[:,0] + df.iloc[:,1] + df.iloc[:,2]\n",
    "\n",
    "#convert data from content field to text\n",
    "txt = df['content'].astype(str)\n",
    "\n",
    "# function to clean the text from stopwords, punctuations and perform lemmatization\n",
    "def cleanText(contentString):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenizedString = tokenizer.tokenize(contentString)\n",
    "    stop = stopwords.words('english')\n",
    "    stopwordsRemovedString = [i for i in tokenizedString if i not in stop]\n",
    "    lemmatizedList = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedString = [wordnet_lemmatizer.lemmatize(i, pos=\"v\") for i in stopwordsRemovedString]\n",
    "    lemmatizedList.append(lemmatizedString)\n",
    "    return lemmatizedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = cleanText(txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(contentString):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenizedString = tokenizer.tokenize(contentString)\n",
    "    return nltk.pos_tag(tokenizedString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pos(txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 11, 'IN': 11, 'JJ': 11, 'CD': 8, 'NNS': 8, 'DT': 6, 'VBZ': 6, 'NNP': 4, 'TO': 3, 'WDT': 3, 'JJR': 3, 'CC': 2, 'PRP': 2, 'VBN': 2, 'RB': 2, 'VBP': 2, 'VB': 1, 'VBD': 1})\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(tag for word,  tag in tags)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = dict((k, v) for k, v in coun_dict.items() if k in ('CD','NN','NNP', 'VB','VBD', 'VBZ','JJ','JJR')  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result = [i for i in tags if i[1] in ('CD','NN','NNP','NNS', 'VB','VBD','JJ','JJR') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.iloc[:,0] + df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To announce the launch of Ultrashine XL 404 which is thinner than other steel sheets and strong.The new 404 sheet from ISC is thinner than other steel sheets, strong and gives you more width for the same weight.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import bigrams, ngrams\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = t[0].lower()\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "ngrams2 = list(ngrams(tokens, 2))\n",
    "ngrams3 = list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = cleanText(t[0])\n",
    "# print(t1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 'announce'),\n",
       " ('announce', 'the'),\n",
       " ('the', 'launch'),\n",
       " ('launch', 'of'),\n",
       " ('of', 'ultrashine'),\n",
       " ('ultrashine', 'xl'),\n",
       " ('xl', '404'),\n",
       " ('404', 'which'),\n",
       " ('which', 'is'),\n",
       " ('is', 'thinner'),\n",
       " ('thinner', 'than'),\n",
       " ('than', 'other'),\n",
       " ('other', 'steel'),\n",
       " ('steel', 'sheets'),\n",
       " ('sheets', 'and'),\n",
       " ('and', 'strong.the'),\n",
       " ('strong.the', 'new'),\n",
       " ('new', '404'),\n",
       " ('404', 'sheet'),\n",
       " ('sheet', 'from'),\n",
       " ('from', 'isc'),\n",
       " ('isc', 'is'),\n",
       " ('is', 'thinner'),\n",
       " ('thinner', 'than'),\n",
       " ('than', 'other'),\n",
       " ('other', 'steel'),\n",
       " ('steel', 'sheets,'),\n",
       " ('sheets,', 'strong'),\n",
       " ('strong', 'and'),\n",
       " ('and', 'gives'),\n",
       " ('gives', 'you'),\n",
       " ('you', 'more'),\n",
       " ('more', 'width'),\n",
       " ('width', 'for'),\n",
       " ('for', 'the'),\n",
       " ('the', 'same'),\n",
       " ('same', 'weight.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b5ecc43caa8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#filtered_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwordcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't1' is not defined"
     ]
    }
   ],
   "source": [
    "#filtered_result\n",
    "wordcounts = Counter(t1[0])\n",
    "print(wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn2 = [t for t in ngrams2 if 'sheet' in t[1]]\n",
    "tn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn3 = [t for t in ngrams3 if 'sheet' in t[2]]\n",
    "tn3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
