{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(\"D:\\\\Study\\\\DataScience\\\\Projects\\\\AI-Advertising\\\\SentimentAnalysis\\\\IMDBDataset.csv\")\n",
    "\n",
    "df.isnull().values.any()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv(\"D:\\\\Study\\\\DataScience\\\\Projects\\\\AI-Advertising\\\\SentimentAnalysis\\\\comments.csv\")\n",
    " \n",
    "#df = df.sample(frac=1)\n",
    " \n",
    "train_sentences = df[\"review\"].fillna(\"CVxTz\").values\n",
    "#list_classes = [\"sentiment\"]\n",
    "\n",
    "train_y = df['sentiment']\n",
    "\n",
    "train_y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, train_y)))\n",
    "\n",
    "#train_y = df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN=128\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    " \n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
    " \n",
    "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    " \n",
    "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
    " \n",
    "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n",
    " \n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_input(sentence,MAX_LEN):\n",
    "  \n",
    "  stokens = tokenizer.tokenize(sentence)\n",
    "  \n",
    "  stokens = stokens[:MAX_LEN]\n",
    "  \n",
    "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
    "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
    "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
    " \n",
    "  return ids,masks,segments\n",
    " \n",
    "def create_input_array(sentences):\n",
    " \n",
    "  input_ids, input_masks, input_segments = [], [], []\n",
    " \n",
    "  for sentence in tqdm(sentences,position=0, leave=True):\n",
    "  \n",
    "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n",
    " \n",
    "    input_ids.append(ids)\n",
    "    input_masks.append(masks)\n",
    "    input_segments.append(segments)\n",
    " \n",
    "  return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
    " \n",
    "model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [04:02<00:00, 206.05it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs=create_input_array(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [03:56<00:00, 211.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n"
     ]
    }
   ],
   "source": [
    "#inputs=create_input_array(train_sentences)\n",
    " \n",
    "model.fit(inputs,train_y,epochs=1,batch_size=32,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv(\"D:\\\\Study\\\\DataScience\\\\Projects\\\\AI-Advertising\\\\SentimentAnalysis\\\\comments_test.csv\")\n",
    " \n",
    "test_sentences = test_df[\"comment_text\"].fillna(\"CVxTz\").values\n",
    " \n",
    "test_inputs=create_input_array(test_sentences[110:150])\n",
    " \n",
    "print(model.predict(test_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
